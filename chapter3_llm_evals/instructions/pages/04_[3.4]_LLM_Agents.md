# [3.4] - LLM Agents


> **Colab: [exercises](https://colab.research.google.com/github/callummcdougall/ARENA_3.0/blob/main/chapter3_llm_evals/exercises/part4_llm_agents/3.4_LLM_Agents_exercises.ipynb?t=20250925) | [solutions](https://colab.research.google.com/github/callummcdougall/ARENA_3.0/blob/main/chapter3_llm_evals/exercises/part4_llm_agents/3.4_LLM_Agents_solutions.ipynb?t=20250925)**

Please send any problems / bugs on the `#errata` channel in the [Slack group](https://join.slack.com/t/arena-uk/shared_invite/zt-3afdmdhye-Mdb3Sv~ss_V_mEaXEbkABA), and ask any questions on the dedicated channels for this chapter of material.

If you want to change to dark mode, you can do this by clicking the three horizontal lines in the top-right, then navigating to Settings → Theme.

Links to all other chapters: [(0) Fundamentals](https://arena-chapter0-fundamentals.streamlit.app/), [(1) Transformer Interpretability](https://arena-chapter1-transformer-interp.streamlit.app/), [(2) RL](https://arena-chapter2-rl.streamlit.app/).

<img src = "https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/robot-typewriter.png" width = "350">


# Introduction


This set of exercises can last up to 2 days, and involves building and working with LLM agents using Inspect. LLM agents consist of a scaffolding program interacting with an LLM API. We'll also build and analyse two tasks for LLM agents to complete, a simple and a complex one, in order to see how LLM agents act.

We'll begin by building a simple Arithmetic Task and Arithmetic Agent. This should teach you the basics of function calling using Inspect. Then, once we're comfortable with function calling and the general setup of LLM agents and tasks, we'll move on to building a more complex agent that plays the [Wikipedia Game](https://en.wikipedia.org/wiki/Wikipedia:Wiki_Game).

Then we'll explore a variety of elicitation methods. These are methods for getting the best capabilities out of models, and are crucial for evaluating LLM agents. Looking at model performance with elicitation method help us to answer the question "Can the model do this?" Unfortunately, we'll almost never be able to *prove* that the model doesn't have a capability, and will only be able to say that *with some effort*, we couldn't get the model to demonstrate this capability. This means we'll have to put a lot of effort into trying to exhibit the behavior in models (to have the highest confidence when we make a claim that the model can't exhibit this behavior). This will involve:

* Improving our prompting
* Improving our tools
* Improving the way the relevant information is stored
* Ensuring the model can access good information

Each exercise will have a difficulty and importance rating out of 5, as well as an estimated maximum time you should spend on these exercises and sometimes a short annotation. You should interpret the ratings & time estimates relatively (e.g. if you find yourself spending about 50% longer on the exercises than the time estimates, adjust accordingly). Please do skip exercises / look at solutions if you don't feel like they're important enough to be worth doing, and you'd rather get to the good stuff!

For a lecture on the material today, which provides some high-level understanding before you dive into the material, watch the video below:

<iframe width="540" height="304" src="https://www.youtube.com/embed/H7hXqm1idAI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


## Content & Learning Objectives

### 1️⃣ Intro to LLM Agents

> ##### Learning Objectives
> - Understand why we want to evaluate LLM agents.
> - Read resources about LLM agent evaluations to understand the current state of the field.
> - Understand the common failure modes of LLM agents.

### 2️⃣ Building a Simple Arithmetic Agent

> ##### Learning Objectives
> - Understand that a LLM agent is just a "glorified for-loop" (of the scaffolding program interacting with the LLM API).
> - Learn how to use function calling to allow LLMs to use external tools.
> - Understand the main functionalities of an LLM agent.

### 3️⃣ Building a more Complex Agent: WikiGame

> ##### Learning Objectives
> - Get comfortable building a more complex task, with noisy and imperfect tool outputs
> - Understand how to build a more complex agent that implements dynamic decision-making
> - Observe the failure modes of a more complex agent

### 4️⃣ Elicitation

> ##### Learning Objectives
> - Understand the importance of elicitation in evaluating LLM agents
> - Understand the different methods of elicitation
> - Understand how to improve prompting, tools, history storage, and information access in LLM agents


```python
import json
import math
import os
import re
import sys
from pathlib import Path
from typing import Any, Literal, Optional
import inspect_ai
from inspect_ai.agent import Agent, AgentState, agent
import inspect_ai.model as inspect_ai_model
from inspect_ai.model import ChatMessageAssistant, ChatMessageUser, ChatMessageSystem, get_model, execute_tools, ChatMessageTool
from inspect_ai.scorer import match
from inspect_ai.dataset import Sample, json_dataset, hf_dataset
from inspect_ai.tool import tool, Tool, ToolCall, tool_with
from inspect_ai.agent import run
from inspect_ai import Task, task, eval
from inspect_ai.agent import as_solver
import wikipedia
from anthropic import Anthropic
from dotenv import load_dotenv
from openai import BadRequestError, OpenAI
from openai.types.chat.chat_completion_message import ChatCompletionMessage
from openai.types.chat.chat_completion_message_tool_call import (
    ChatCompletionMessageToolCall,
)
from wikipedia import DisambiguationError, PageError, WikipediaPage

# Make sure exercises are in the path
chapter = "chapter3_llm_evals"
section = "part4_llm_agents"
root_dir = next(p for p in Path.cwd().parents if (p / chapter).exists())
exercises_dir = root_dir / chapter / "exercises"
section_dir = exercises_dir / section
if str(exercises_dir) not in sys.path:
    sys.path.append(str(exercises_dir))

import part4_llm_agents.tests as tests
from part1_intro_to_evals.solutions import retry_with_exponential_backoff
from utils import countrylist, evaluate_expression, wiki_pairs, execute_tools, extract_answer
EVAL_MODEL = "openai/gpt-4o-mini"
os.environ["INSPECT_EVAL_MODEL"] = EVAL_MODEL
MAIN = __name__ == "__main__"
```


```python
load_dotenv()

assert os.getenv("OPENAI_API_KEY") is not None, (
    "You must set your OpenAI API key - see instructions in dropdown"
)

# OPENAI_API_KEY

openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
```




=== NEW CHAPTER ===


# 1️⃣ Intro to LLM Agents



> ##### Learning Objectives
>
> - Read resources about LLM agent evaluations to understand the current state of the field.
> - Understand the common failure modes of LLM agents.

## What is an LLM agent?

An LLM agent consists of a scaffolding program interacting with an LLM API to accomplish tasks in an external environment. This typically involves a loop of the following steps:

1. The scaffolding program sends instructions to the LLM, typically containing information about the task goal, the possible actions available to the LLM, and any other relevant task information. (e.g. you are trying to calculate `3+3`)
2. The LLM processes the input and outputs an action in text (e.g. "calls" the `calculate()` tool on the expression `3+3` in text).
3. The scaffolding program executes the action and returns the outcome (e.g. it runs the `calculate()` function in the background and returns the output, `6`, to the agent).
4. The LLM observes the results and decides the next action.
5. Repeating the cycle until the task is complete.

The two basic components of scaffolding are:

* Tool calling: This allows LLMs to use tools by providing a text description of the tool. The LLM can choose to use this tool by "calling" it in its text output. If it uses a tool, the scaffolding will execute this tool on the LLM's behalf (e.g. by running a python function, sending request to an external API etc.) and return the result of this tool call to the agent.
* Prompting: This describes the task state to the LLM, describes the tools available to the LLM in the task, potentially instructs the LLM to use chain-of-thought to give the LLM more "thinking time." This also covers how the LLM's `chat_history` is stored from the LLM's prior actions.


<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/refs/heads/main/img/ch3-llm-agent.png" width="800">

Diagram based on METR's [*Evaluating Language-Model Agents on Realistic Autonomous Tasks*](https://arxiv.org/abs/2312.11671), Figure 2.


## Why evaluate LLM agents?

There are at least two reasons we want to evaluate LLM agents.

1. **Measuring the maximum capabilities of a model**

For estimating safety risks, we want to measure the **ceiling** of dangerous capabilities. LLMs on their own often fail in easy-to-fix ways, as you will see. For example:

- They often claim to be incapable of tasks that they can actually perform.
- They can *very* easily get stuck in loops.
- They can give up and ask the user for help
- They can hallucinate facts, or even misunderstand their own prior reasoning and hallucinate a faulty conclusion.
- They can be limited by primitive tools.
- They can be sensitive in strange ways to information in their prompts.
- They can have bugs, typos, or other minor barriers that prevent them from operating to the fullest extent of their capability.

This means that when a model fails to accomplish a task, it may still have the capability to succeed; requiring only simple fixes that will unlock this capability. We want to eliminate the possibility of large capability improvements from relatively little effort, because this means our evaluation would have underestimated the true capability and risks associated with a model (especially in e.g. a dangerous capabilities evaluation). Therefore, we to try hard to elicit their raw capabilities (e.g. using scaffolding), so that we can evaluate LLMs at their *best*.


2. **Measuring the alignment of LLMs in agentic scenarios**

We do not know if our current alignment techniques (e.g. supervised fine-tuning, RLHF) for aligning LLM chatbots will still work when LLMs are acting as agents in more complex scenarios. It is possible that these methods will not generalize well to agentic scenarios, and we may want to test this.

We know today that LLMs are being used as more than just chatbots. Since the release of ChatGPT, the use of LLMs as agentic systems has grown signifcantly. These agents started off rather disappointingly, when they were based on GPT-3.5. However as more powerful LLMs come out and AI companies ensure their LLMs are better at tool-use, these agents are improving rapidly.

<details><summary>Further resources on LLM agent evaluations</summary>

- [Evaluating Language-Model Agents on Realistic Autonomous Tasks](https://evals.alignment.org/Evaluating_LMAs_Realistic_Tasks.pdf) (Kinniment et al., ARC Evaluations Team (now METR), 2023)
- [Large Language Models can Strategically Deceive their Users when Put Under Pressure](https://arxiv.org/pdf/2311.07590) (Scheurer et al., Apollo Research, ICLR 2024)
- [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) (Lilian Weng, OpenAI Safety Team, 2023)
- [AXRP Episode 34 - AI Evaluations with Beth Barnes](https://www.alignmentforum.org/posts/vACr4DExfeRMaCoo7/axrp-episode-34-ai-evaluations-with-beth-barnes) (Daniel Filan, 2024)
-[Reflexion: Language Agents with Verbal Reinforcement Learning](https://arxiv.org/pdf/2303.11366) (Shinn et al., 2023)
- [Answering Questions by Meta-Reasoning over Multiple Chains of Thought](https://arxiv.org/pdf/2304.13007) (Yoran et al., 2024)
- [Toolformer: Language Models Can Teach Themselves to Use Tools](https://arxiv.org/pdf/2302.04761) (Schick et al., META AI Research, 2023)
- [OpenAI Function Calling Guide](https://platform.openai.com/docs/guides/function-calling)
- [Anthropic Function Calling Guide](https://docs.anthropic.com/en/docs/build-with-claude/tool-use)

</details>




=== NEW CHAPTER ===


# 2️⃣ Building a Simple Arithmetic Agent



> ##### Learning Objectives
>
> - Learn how to use function calling to allow LLMs to use external tools.
> - Understand the main functionalities of an LLM agent.

In general, most LLM agents share these core components:

<img src="https://raw.githubusercontent.com/chloeli-15/ARENA_img/refs/heads/main/img/ch3-sec4-agent-overview.png" width="1000">


1. **LLM API interface**: A basic function that makes API calls (e.g. `generate()`). <!-- (IN AGENT)-->
2. **Actions/Tools**: A set of actions the agent can take. <!-- (MOSTLY IN TASK)-->
3. **Task State Management**: Keeping track of the current state of the task and any relevant context. <!-- (IN TASK MOSTLY)-->
4. **Memory**: A way to store and retrieve information from past interactions (i.e. chat history). In inspect, we store it in `state.messages`. <!-- (IN AGENT)-->
5. **Observation Parser**: Functions to parse and interpret the results of actions and update the state. <!-- (IN TASK/TOOLS MOSTLY)-->
6. **Decision/Execution Logic**: The rules or algorithms used to choose actions based on the current state and LLM output. <!-- (MOSTLY IN AGENT)-->
7. **Task-Specific Information**: Any additional information or functions specific to the task at hand. <!-- (INFO IN AGENT/FUNCTIONS IN TASK)-->

These components are implemented across the `Task`, `Agent`, and `Tool` functions/classes. However, the specific breakdown of these components in our implementation is a design choice and can vary depending on the task. While some are very natural (e.g. LLM API interface goes into `Agent`, task state management goes into `Task`), others can vary (e.g. `Tool`s could be implemented and handled entirely within the `Task` or `Agent`, as opposed to being separate functions; observation parsing could be in the `Task` or the `Agent` class). In general, we want to maximize separability and minimize interfaces/dependencies, so that we can easily swap out different agents for the same task, or vice versa.


## Task

In an LLM agent eval, there will usually be a `Task` class that interacts with the `Agent`. In general, the `Task` will:

- Prepare and provide the task instruction (and necessary files, functions etc) to the agent
- Parse and score the agent's output
- Update the task state accordingly (e.g. proceeds onto the next step of the task, ends the task).


### Exercise - Build a simple arithmetic task
> ```yaml
> Difficulty: 🔴🔴🔴⚪⚪
> Importance: 🔵🔵⚪⚪⚪
> 
> You should spend up to 20-25 minutes on this exercise.
> ```

First build a toy task called `ArithmeticTask`. This task should take in two numbers and create a list of arithmetic calculation problems with these two numbers, using the binary arithmetic operations defined in `ArithmeticTask.operations`. It should have methods to do the following:

- Output the current problem (e.g. at the start this will be "Calculate `num1 + num2`");
- Generate and store the correct answers to the problems
- Move to the next problem if the model answer was correct, (or if the model refuses to answer the question);
- Output the instruction for the current problem.
- Check if all the problems have been solved.

**How to handle calculations?** We have implemented a helper function `evaluate_expression()` to evaluate the arithmetic expressions, which you should use in your implementation of `execute()`. `evaluate_expression()` takes an arithmetic expression as a string (e.g. "3+5") and returns the result as a string (e.g. "8.0").

<details><summary>Aside: Why not use Python's in-built <code>eval()</code> function?</summary>

Python's `eval()` function evaluates an arbitrary string expression, and so allows AI models to run arbitrary code. Unless you have set up a container or sandboxed environment, it is very bad practice to allow LLMs to run arbitrary code on your computer!

</details>


```python
class ArithmeticTask:

    def __init__(self, num1: int | float, num2: int | float, operations: Optional[list[str]] = None):
        self.num1 = num1
        self.num2 = num2
        self.operations = operations if operations else ["+", "-", "*", "/", "**", "//", "%"]
        self.current_task_number = 0

    def _generate_answers(self) -> list[str]:
        """
        Generates a list of the correct answers for all the possible tasks

        Returns:
            list[str]: A list of the correct answers for all the possible tasks
        """
        raise NotImplementedError("You need to implement _generate_answers")
    
    @property
    def get_current_task(self) -> str:
        raise NotImplementedError("You need to implement get_current_task")
    def update_current_task(self) -> None:
        """
        Increments self.current_task_number by one (modulo the number of operations)
        """
        raise NotImplementedError("You need to implement update_current_task")

    def get_current_instruction(self) -> ChatMessageUser:
        raise NotImplementedError("You need to implement get_current_instruction")

arithmetic_task1 = ArithmeticTask(3, 5)
print(arithmetic_task1.get_current_task)
arithmetic_task1.update_current_task()
print(arithmetic_task1.get_current_task)
print(arithmetic_task1.get_current_instruction())
```


<details><summary>Click to see the expected output</summary>

<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">3 + 5
3 - 5
id='mcqhxvsD2B9T92JLTP7GBm' content='Calculate the following expression 3 - 5. Give your answer in the format <ANSWER>NUMBER</ANSWER> where NUMBER is a numerical value formatted as a float.' source=None metadata=None role='user' tool_call_id=None</pre>

</details>


<details><summary>Aside - What is <code>@property</code>?</summary>

The `@property` decorator in python is used to define methods that behave like they were attributes.

1. It allows you to access a method as though it were an attribute, without parentheses.
2. It allows you to perform functions when calling attributes, e.g. adding validation or performing any necessary calculations (in our case incorporating class attributes which frequently change).

For example, if we defined a `Square` class as follows:

```python
class Square:
    def __init__(self, side_length):
        self.side_length = side_length

    @property
    def perimeter(self):
        return self.side_length*4
```

Then we could access `perimeter` as if it were an attribute:

```python 
s = Square(4)
print(s.perimeter) # Output: 16
```

Using `@property` in this case helps with:
1. Making the intent of the code clearer
2. Making it slightly easier to access these "properties" of the class

</details>

<details><summary>Solution</summary>

```python
class ArithmeticTask:

    def __init__(self, num1: int | float, num2: int | float, operations: Optional[list[str]] = None):
        self.num1 = num1
        self.num2 = num2
        self.operations = operations if operations else ["+", "-", "*", "/", "**", "//", "%"]
        self.current_task_number = 0

    def _generate_answers(self) -> list[str]:
        """
        Generates a list of the correct answers for all the possible tasks

        Returns:
            list[str]: A list of the correct answers for all the possible tasks
        """
        answers = []
        for op in self.operations:
            try:
                result = evaluate_expression(f"{self.num1} {op} {self.num2}")
                answers.append(str(result))
            except Exception as e:
                answers.append(f"Error: {str(e)}")
        return answers
    
    @property
    def get_current_task(self) -> str:
        return f"{self.num1} {self.operations[self.current_task_number]} {self.num2}"
    def update_current_task(self) -> None:
        """
        Increments self.current_task_number by one (modulo the number of operations)
        """
        self.current_task_number = (self.current_task_number + 1) % len(self.operations)

    def get_current_instruction(self) -> ChatMessageUser:
        return ChatMessageUser(content= f"Calculate the following expression {self.get_current_task}. Give your answer in the format <ANSWER>NUMBER</ANSWER> where NUMBER is a numerical value formatted as a float.")

arithmetic_task1 = ArithmeticTask(3, 5)
print(arithmetic_task1.get_current_task)
arithmetic_task1.update_current_task()
print(arithmetic_task1.get_current_task)
print(arithmetic_task1.get_current_instruction())
```

</details>


## Tool use via function calling

The simplest way for LLMs to take actions is via function calling. **Function calling** is a built-in feature of many LLM APIs that allows models to use external "tools" (i.e. Python functions, APIs) by simply receiving and outputting text. This involves 5 simple steps:

1. Pick a function in your codebase that the model should be able to call
2. Describe your function in the syntax of the model's API so the model knows how to call it
3. Pass your function definitions as available “tools” to the model, along with the messages
4. Receive and handle the model response
5. Provide the function call result back to the model 

**This loop of prompting the LLM with tools, executing its actions, and returning the results forms the basis of all LLM agents.** It allows LLMs to perform complex tasks like playing a game or completing a coding project "autonomously".

We will implement each step of the loop below.


### Exercise - Write `CalculateTool`
> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵🔵🔵⚪
> 
> You should spend up to 10-15 minutes on this exercise.
> ```

We will define a tool class for our simple `calculate()` function with the following structure (you don't need to run this code):

```python
@tool 
def tool_name(function_args):
    async def execute(tool_args : type) -> Tool:
        # tool logic
        return tool_output
    return execute
```

For the `CalculateTool`, you should implement `execute()`, which should take in an arithmetical expression as string (e.g. `"3+5"`) and return the result of this expression (also as a string).

#### Tool Description

You also need to make sure that your docstring for the tool is formatted correctly, as inspect reads the docstring in order to determine what information about the tool is passed to the LLM. Models like ChatGPT and Claude are fine-tuned to interpret and respond to `tool` descriptions appropriately, just like `user` and `system` messages.


To understand what's being fed to the LLM API, below is an example of a typical tool description for the OpenAI API (see their [function calling guide](https://platform.openai.com/docs/guides/function-calling) for more details). Note that this may differ slightly between APIs.

```python
{
    "type": "function",
    "function": {
        {
            "name": "get_delivery_date",
            "description": "Get the delivery date for a customer's order. Call this whenever you need to know the delivery date, for example when a customer asks 'Where is my package'",
            "parameters": {
                "type": "object",
                "properties": {
                    "order_id": {
                        "type": "string",
                        "description": "The customer's order ID.",
                        },
                    },
                "required": ["order_id"],
                "additionalProperties": false,
            },
        },
    },
},
```

For Inspect, you should make sure that the *types* of the arguments are included, and that the docstring is formatted as given below. Here's a toy example of a tool that simply takes a number and a string as arguments, and appends the number to the end of the string.

```python
@tool 
def tool_name():
    def execute(sentence : str, n : int) -> str:
        """
        This tool appends a number to the end of the string.

        Args:
            sentence: this is the string to which you want to append a number.
            n: this is the number you want to append.
        
        Returns: 
            The sentence with the number appended.
        """
        return sentence + str(n)
```

Inspect will then find the argument types, descriptions, and the tool description and create a formatted tool description for us. E.g. in the above case, the tool description would look like:

```c
{
    "type": "function",
    "function": {
        {
        "name": "tool_name",
        "description": "This tool appends a number to the end of the string.",
        "parameters": {
            "type": "object",
            "properties": {
                "sentence": {
                    "type": "string",
                    "description": "this is the string to which you want to append a number.",
                    },
                    "n":{
                    "type": "integer",
                    "description": "this is the number you want to append."
                    },
                },
            "required": ["sentence", "n"],
            "additionalProperties": false,
            },
        },
    },
}
```
            
<details><summary><b>Good practices for writing tool descriptions</b></summary>

Here are some good practices for writing tool descriptions for Claude according to Anthropic, which should generalize to other chat models:
- Provide extremely detailed descriptions. This is by far the most important factor in how effectively the model uses the tool. Your descriptions should explain every aspect of the tool, including:
    - What the tool does
    - When it should be used (and when it *shouldn’t*, if this confuses the model)
    - What each parameter means and how it affects the tool’s behavior
    - Any important caveats or limitations, such as what information the tool does not return if the tool name is unclear. 
- Prioritize descriptions over examples. While you can include examples of how to use a tool in its description or in the accompanying prompt, this is less important than having a clear and comprehensive explanation of the tool’s purpose and parameters. Only add examples after you’ve fully fleshed out the description.

The more context you can give models about your tools, the better it will be at deciding when and how to use them. Aim for at least 3-4 sentences per tool description, more if the tool is complex.

Read Anthropic's examples of what good and bad tool calling looks like [here](https://docs.anthropic.com/en/docs/build-with-claude/tool-use#example-of-a-good-tool-description). 

</details>

Write your tool function for the `calculate()` tool below.


```python
@tool
def calculate(): 
    async def execute(expression : str) -> str:
        raise NotImplementedError("You need to implement the calculate tool and make sure the docstring is formatted correctly")
    return execute
```


<details><summary>Solution</summary>

```python
@tool
def calculate(): 
    async def execute(expression : str) -> str:
        """
        A calculator that can evaluate arithmetic expressions. The input is a mathematical expression, as a string, and the output is the result of the calculation.

        Args:
            expression: the arithmetic expression to evaluate.

        Returns: 
            The result of the calculation, as a string. Or error if the expression is invalid.
        """
        try: 
            return str(evaluate_expression(expression))
        except Exception as e:
            return f"Error: {e}"
    return execute
```
</details>


Outside of inspect, tools are passed to the model by inserting the tool description in the `tools` parameter of the API call.


## Agent

We will now implement an `ArithmeticAgent` class that is not specific to the `ArithmeticTask`, so that we can see the key components of a generic LLM agent.


### Exercise - Build an `ArithmeticAgent`

> ```yaml
> Difficulty: 🔴🔴🔴🔴⚪
> Importance: 🔵🔵🔵🔵🔵
> 
> You should spend up to 20-25 minutes on this exercise.
> ```

Now build our agent that will interact with the `ArithmeticTask` (with a calculator tool). You should implement the `execute()` function so that it runs the task in its entirety. For more complicated tasks, we'll implement a more modular agent design.

You should use inspect's `generate()` function in order to get the model to use tools and generate a response (you can pass the tools to `generate()` via the `tools` argument). If the model decides to call a tool, you should use inspect's `execute_tools` function in order to run the tool. Finally, you should check whether the model's answer is correct or not. We've provided a helper function that extracts answers wrapped in `"<ANSWER></ANSWER>"` tags from the content of the agent's response, accessible by `utils.extract_answer()`. 

Then if the answer is correct, you should update the task so that the model moves on to the next problem. Finally, if all the answers are solved correctly, the loop should stop, as the model has accomplished its task.

It may be useful for this exercise (and throughout this section) to refer to Inspect's agent docs [here](https://inspect.aisi.org.uk/agent-custom.html)


```python
@agent
def arithmetic_agent(task : ArithmeticTask):

    async def execute(state: AgentState) -> AgentState:
        raise NotImplementedError("You need to implement the arithmetic_agent")
    return execute
```


<details><summary>Solution</summary>

```python
@agent
def arithmetic_agent(task : ArithmeticTask):

    async def execute(state: AgentState) -> AgentState:
        answer_list = ["wrong"] * len(task.operations)
        success = False
        while not success:
            state.messages.append(task.get_current_instruction())
            state.output = await get_model().generate(input = state.messages, tools = [calculate()], tool_choice = "auto")
            state.messages.append(state.output.message)
            if state.output.message.tool_calls:
                messages, state.output = await execute_tools(state.messages, tools = [calculate()])
                state.messages.extend(messages)
            state.output = await get_model().generate(input = state.messages, tools = [calculate()], tool_choice = "none")
            state.messages.append(state.output.message)
            try:
                if extract_answer(state.output.message.content) == task._generate_answers()[task.current_task_number]:
                    answer_list[task.current_task_number] = extract_answer(state.output.message.content)
                    task.update_current_task()

                else:
                    state.messages.append(ChatMessageUser(content="Incorrect answer. Try again."))
            except IndexError:
                state.messages.append(ChatMessageUser(content="Error: Could not extract answer"))
            if all(ans == task._generate_answers()[i] for i, ans in enumerate(answer_list)):
                success = True
        return state 
    return execute
```
</details>


### Exercise - Use eval to run the agent

> ```yaml
> Difficulty: 🔴⚪⚪⚪⚪
> Importance: 🔵🔵🔵🔵⚪
> 
> You should spend up to 5-10 minutes on this exercise.
> ```

Now let's use inspect's `eval()` function to run the agent. We can turn our agent into a "solver" using Inspect's `as_solver` function. We don't need to pass a `Solver` in this case, as we'll mostly be reading agent output. However, since there is a clear cut goal, you can write a solver if you'd like. Refer back to [section 3.3, Scorers](https://arena-chapter3-llm-evals.streamlit.app/[3.3]_Running_Evals_with_Inspect#scorers) if you don't remember how to do this. (We'll pass in an AgentState instead of a TaskState in this case).

> **WARNING!** 
>
>When you're making API calls to LLMs to accomplish longer tasks, it can be tempting to use a while loop, and run the model until it finishes the task. But since every time we run a model we make an API call, this would allow us to spend arbitrarily large amounts of money on API calls. For this reason, ***always set a limit to your loop when making API calls*** It would be really unfortunate if you blew all your API budget on one mistake.
>
> Inspect allows us to set a `message_limit` that restricts how many messages can occur in the conversation. This will be useful to ensure the model doesn't run for too long.


```python
@task
def agent_task() -> str:
    return Task(dataset = [Sample(input = "", target = "")], message_limit=40)

eval(agent_task(), solver = as_solver(arithmetic_agent(task = ArithmeticTask(3, 5))))
```




=== NEW CHAPTER ===


# 3️⃣ Building a more complex agent: WikiGame



> ##### Learning Objectives
>
> - Understand how to build a more complex agent that implements dynamic decision-making
> - Observe the failure modes of a more complex agent

The task in this section simulates the Wikipedia game, where the agent starts on one Wikipedia page and attempts to navigate to a goal page using only links found in the main content of Wikipedia pages. Compared to the previous sections, the main challenge here is to implement **dynamic decision making while parsing noisy and imperfect outputs**.


## Quick intro to the Wikipedia API

Our agent will interact with Wikipedia by making tool calls to the [Wikipedia API](https://wikipedia.readthedocs.io/en/latest/quickstart.html). We will only need to learn the following key functions from the Wikipedia API to be able to implement the basic dynamics of the game. 

1. `wikipedia.page()` - Returns a `WikipediaPage` object, which contains various attributes and methods to access page content. (See [page docs](https://wikipedia-api.readthedocs.io/en/latest/API.html#wikipediapage) for these attributes.)
2. `wikipediaPage.title` - Returns the title of the page
3. `wikipediaPage.content` - Returns the full text content of the page (this can be very long, make sure to take snippets when possible to not use up the context window of the LLM)
4. `wikipediaPage.summary` - Returns a summary of the page (i.e. the introductory text of the Wikipage before the first section title).
5. `wikipediaPage.links` - Returns a list of all links as strings

<details><summary> Aside: Wikipedia API content can be weird!</summary>

The wikipedia API often outputs content in unintuitive ways. For example, articles that are essentially just a big list become near useless, since the content usually omits the list (for example, see the wikipedia API content for <a href = "https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population">List of countries and dependencies by population</a>). Another issue that you might encounter is that the API formats mathematical expressions in $\LaTeX$ quite poorly (for example, see the wikipedia API content for <a href = "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Kullback-Leibler divergence</a>). This is why it's important to determine what content the wikipedia API produces when `.content` is called — and why you want to make sure you're testing a large diversity of wikipedia articles.

</details>

<details><summary> Aside: Wikipedia "summaries" can be long!</summary>

The wikipedia API accesses summaries of pages by presenting all the information before the first titled section. For certain (generally obscure) wikipedia pages, this summary itself can be extremely long, and contain lots of information that is unnecessary to determine the key information about the page the model should be trying to access. We'll handle this later when it comes up by truncating wikipedia's summary to just the first ~500 characters

</details>

Run the following code to see how these wikipedia API functions work!


```python
# Retrieve a Wikipedia page from its title
page = wikipedia.page("Large language model")

# Access basic page information
print("Title:", page.title)
print("\nURL", page.url)
print(f"\nSummary (word count {len(page.summary.split())}):", page.summary)
print(
    f"\nContent (word count {len(page.content.split())}):",
    page.content[:1000],
    "......",
)
print(f"""\nLinks (link count {len(page.links)}): [{", ".join(page.links[:7])}, ......]""")
```


<details><summary>Click to see the output of this code (the wikipedia page might have changed slightly)</summary>

<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Title: Large language model

URL https://en.wikipedia.org/wiki/Large_language_model

Summary (word count 95): A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.
The largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.


Content (word count 6887): A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.
The largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.


== History ==

Before 2017, there were a few language models that were large as compared to capacities then available. In the 1990s, the IBM alignment models pioneered statistical language modelling. A smoothed n-gram model in 2001 trained on 0.3 billion words achieved state-of-the-art perplexity at the time. In the 2000s, as Internet use became prevalent, some rese ......

Links (link count 524): [15.ai, AI-complete, AI explainability, API, Action selection, Activation function, Active learning (machine learning), ......]
</pre>

</details>


The following two cell blocks cause an error when run (you should see a `DisambiguationError` for the first, and a `PageError` for the second). These are common errors that LLMs can encounter when moving between wikipedia pages, and so we'll need to find a way to handle them:


```python
try:
    page = wikipedia.page("Python")
except DisambiguationError as e:
    print(type(e), "\n\n", e)
```


```python
try:
    page = wikipedia.page("Animalss", auto_suggest=False)
except Exception as e:
    print(type(e), "\n\n", e)
```


<details><summary>Click to see the output of this code</summary>

<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">&lt;class 'wikipedia.exceptions.PageError'&gt;

 Page id "Animalss" does not match any pages. Try another id!
</pre>

</details>


```python
# Fixes PageError by allowing redirects
page = wikipedia.page("Animalss", redirect=True)
print(page.title)

# Fixes DisambiguationError by selecting the first option

try:
    page = wikipedia.page("Python")
except DisambiguationError as e:
    page = wikipedia.page(e.options[0])
print(page.title)
```


The errors above are:

- `DisambiguationError`: This was raised because the title "Python" can correspond to multiple pages. Whenever this error is raised, we get a list of options that Wikipedia suggests we *could* mean, and so we choose the first.

- `PageError`: This was raised for "Animalss" as there is no Wikipedia page with that title. We can usually avoid these by setting `redirect = True` and allowing Wikipedia to redirect us.

We have implemented a simple function `get_page()` for you to get the `WikipediaPage` object for a particular page title with error handling.


```python
def get_page(title: str) -> WikipediaPage:
    """
    Get a Wikipedia page object given a title. If the title is ambiguous, choose the first option.
    If the title is not found, try to find a similar title.

    Args:
        title (str): The title of the Wikipedia page

    Returns:
        WikipediaPage: The Wikipedia page
    """
    try:
        return wikipedia.page(title, auto_suggest=False, redirect=True)
    except DisambiguationError as e:
        return wikipedia.page(e.options[0], auto_suggest=False, redirect=True)
    except PageError:
        return wikipedia.page(title, auto_suggest=True, redirect=True)
```


<details><summary>What do the kwargs <code>redirect</code> and <code>auto_suggest</code> in <code>wikipedia.page()</code> do?</summary>

`redirect`

- This kwarg enables redirecting when you reference an article title with **slight** differences to how it is stored in Wikipedia. For example, the Wikipedia API will generally access the correct page if there is a capitalization error on the first letter, but not for capitalization errors in the middle of the word if `redirect = False`:
```python
# This returns a WikipediaPage object for the "Human" page
page = wikipedia.page("huMan", redirect = True, auto_suggest=False)

# This raises a PageError since there is no page called "huMan"
page = wikipedia.page("huMan", redirect=False, auto_suggest=False)
```
- By default, we should set `redirect = True` in the `wikipedia.page()` function.

`auto_suggest`

- This kwarg enables the API to provide suggestions. This allows a lot more than `redirect` does, since `redirect` is only for the "obvious" cases (e.g. "huMan" → "Human", "U.S. President" → "President of the United States", etc.). When `auto_suggest` is true, it would allow something like "president of states" → "President of the United States", "gogle" → "Google"; both of which would raise an error if `redirect = True, auto_suggest = False`.

- However, `auto_suggest` can sometimes be *too* permissive and lead to errors. For example, the below code will return a `WikipediaPage` object for the "Man" page. This is clearly not what we were trying to access, and the `auto_suggest` has gotten carried away in this case:

```python
page = wikipedia.page("Human", redirect= False, auto_suggest=True)
```

- If `redirect = True` and `auto_suggest=True`, then `auto_suggest` takes priority. 
- **By default, we should set `auto_suggest` to `False` unless it is used as a last resort to resolve an error!**

</details>


### Exercise - Implement `get_permitted_links()`
> ```yaml
> Difficulty: 🔴⚪⚪⚪⚪
> Importance: 🔵🔵⚪⚪⚪
> 
> You should spend up to ~10 mins on this exercise.
> ```

This is a quick exercise to familarize you with the Wikipedia API.

When you get the links from a page using `page.links`, this will include every possible Wikipedia link that is accessible from the HTML on that page, including those that are not in the main page content (e.g. links in sidebars, links in footnotes etc.), which are irrelevant or not permitted by the rules of the Wiki game. 

Write a simple `get_permitted_links()` function. This should only return the links that can be found inside the main content. The resulting list of permitted links should be about a third as long as the list of links from `page.links` (although it varies slightly by page).


```python
def get_permitted_links(current_page: WikipediaPage) -> list[str]:
    """
    Get "permitted" links (i.e. links that are in the content of the page) from a Wikipedia page.

    Args:
        current_page (WikipediaPage): The current Wikipedia page

    Returns:
        list[str]: A list of permitted links from current_page

    """
    raise NotImplementedError("You need to implement the get_permitted_links function")


tests.test_get_permitted_links(get_permitted_links)
```


<details><summary>Solution</summary>

```python
def get_permitted_links(current_page: WikipediaPage) -> list[str]:
    """
    Get "permitted" links (i.e. links that are in the content of the page) from a Wikipedia page.

    Args:
        current_page (WikipediaPage): The current Wikipedia page

    Returns:
        list[str]: A list of permitted links from current_page

    """
    all_links = current_page.links
    content_lower = current_page.content.lower()
    permitted_links = [link for link in all_links if link.lower() in content_lower]
    if current_page.title in permitted_links:
        permitted_links.remove(current_page.title)
    return permitted_links
```
</details>


## LLM Agent for WikiGame

<!-- Remake diagram -->


### The WikiGame class

Below is the `WikiGame` class that instantiates the Wikipedia game. This class contains the following functionalities:

1. Keeps track of task state variables (e.g. the current page, the page history)
2. Task-specific helper functions for calling the Wikipedia API. 

The implementation of this class has been provided for you, but you should read and understand how they're being implemented, and what they do:

- `WikiGame` initialises with 4 variables:
    - `starting_page` which gives the page that the agent should begin on.
    - `goal_page` which gives the page that the agent should aim to get to.
    - `current_page` which tracks the current page that the agent is on.
    - `page_history` which tracks all the pages the agent has visited in the game (initially only the `starting_page`).

It also comes with 4 methods:
- `get_page()` which takes the title of a Wikipedia page as a string, and returns the `WikipediaPage` object associated to this tile (this is the same as the `get_page()` function we introduced earlier).
- `get_permitted_links()` which gets the permitted links from `WikiGame.current_page`. You can replace this function with your solution to the `get_permitted_links` exercise earlier if you prefer.
- `is_permitted_link()` which takes a link name, and returns `True` if this link is a permitted link, and `False` otherwise.
- `check_win()` which returns `self.current_page == self.goal_page`, corresponding to whether the agent has won the game.


#### Providing information to the agent

While models are trained on most of the Wikipedia content, a particular page may still be confused with something else, or be an article that was added after the training cutoff. Models also can't generally recall information in their training data if they only come up once or twice (as is often the case for obscure wikipedia articles). So you should use the game's `get_summary()` function to provide details of the goal page to the agent in its initial message.


```python
class WikiGame:
    def __init__(
        self,
        starting_page: str,
        goal_page: str,
    ):
        """
        This task simulates the Wikipedia game, where the agent starts on one Wikipedia page and
        attempts to navigate to a goal page using only links found in the main content of Wikipedia
        pages.

        Args:
            starting_page (str): The page the agent starts on.
            goal_page (str): The page the agent is trying to reach.

        Attributes:
            page_history (list[str]): The history of pages visited by the agent.
            starting_page (WikipediaPage): The starting page of the game.
            goal_page (WikipediaPage): The goal page of the game.
            current_page (WikipediaPage): The current page the agent is on.

        """
        self.page_history: list[str] = [starting_page]
        self.starting_page: WikipediaPage = self.get_page(starting_page)
        self.goal_page: WikipediaPage = self.get_page(goal_page)
        self.current_page: WikipediaPage = self.starting_page

    # ========================= Helper Functions (given) =========================

    # Get page and page summary
    @staticmethod
    def get_page(title: str) -> WikipediaPage:
        """
        Get a Wikipedia page object given a title. If the title is ambiguous, choose the first
        option. If the title is not found, try to find a similar title.

        Args:
            title (str): The title of the Wikipedia page

        Returns:
            WikipediaPage: The Wikipedia page
        """
        try:
            return wikipedia.page(title, auto_suggest=False, redirect=True)
        except DisambiguationError as e:
            return wikipedia.page(e.options[0], auto_suggest=False, redirect=True)
        except PageError:
            return wikipedia.page(title, auto_suggest=True, redirect=True)

    def get_page_summary(self, page: WikipediaPage | None = None) -> str:
        """
        Get summary of a wikipedia page, to the last full stop within the first 500 characters.
        This can be used to give a brief overview of a page to the agent.

        Args:
            page (WikipediaPage): The Wikipedia page object.

        Returns:
            str: The summary of the Wikipedia page.
        """
        page = page if page else self.goal_page
        summary = page.content[:500]
        last_period_index = summary.rfind(".")
        return summary[: last_period_index + 1] if last_period_index != -1 else summary

    # Get and check permitted links
    def get_permitted_links(self) -> list[str]:
        """
        Returns a list of permitted links (i.e. links in the main page content) for the current page.

        Returns:
            list[str]: The permitted links.
        """
        all_links = self.current_page.links
        content_lower = self.current_page.content.lower()
        permitted_links = [link for link in all_links if link.lower() in content_lower]
        if self.current_page.title in permitted_links:
            permitted_links.remove(self.current_page.title)
        return permitted_links

    def is_permitted_link(self, link: str) -> bool:
        """
        Returns True if the link is in the permitted links for the current page, False otherwise.

        Args:
            link (str): The link to check.

        Returns:
            bool: True if the link is permitted, False otherwise
        """
        return link.lower() in (x.lower() for x in self.get_permitted_links())

    # ========================= Task State Management (given) =========================

    def check_win(self) -> bool:
        return self.current_page == self.goal_page
```


### Exercise - Build tools for the WikiGame
> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵🔵⚪⚪
> 
> You should spend up to 15-20 mins on this exercise.
> ```

The basic WikiAgent will need these two tools to play the game:
1. `GetContentTool`: This returns the full content of the current page, with all the wikipedia links wrapped in `<link></link>` tags (as otherwise they are presented as strings and indistinguishable from normal text). Implementing this involves dealing with annoying regex, so we've provided the regex necessary to wrap links with link tags in the hint below. If you'd like an extra challenge, you can try and work it out yourself, but it's *really* not crucial to understanding any of the content today.

2. `MovePageTool`: This executes moving to a given new page when called and updates the `WikiGame` task state if successful. You should implement both the `execute()` function and the `description()` property.

When formatting this tool list, refer back to your code for the arithmetic game, or the OpenAI function-calling docs [here](https://platform.openai.com/docs/guides/function-calling).

<details><summary>Why not just use <code>WikipediaPage.links()</code> to get a list of links directly?</summary>

We don't just present a list of the accessible links, as this is not very faithful to the wikipedia game. The agent does perform somewhat better if we just give it a list of links, but the task of parsing the content of wikipedia pages and isolating the most important links is big part of the challenge of the wikipedia game.

</details>

<details><summary>Caveat for the <code>GetContentTool</code></summary>

The `GetContentTool` wraps all the texts that correspond to links in `<link></link>` tags. However, since we identify links in the text via their names on wikipedia pages, there are certain articles that will never (or only very rarely) get flagged as links. For example, the page "Python (programming language)" is almost never referenced by its title, instead its almost always referenced as just "Python"; the same is true for cities and towns, which often have names such as e.g. "Juneau, Alaska", but these are almost always referred to as just "Juneau" in the articles where they appear. For this reason, you should avoid having goal pages which are likely to be referenced by a different string than their title.

</details>


```python
@tool
def GetContentTool(game : WikiGame) -> Tool:
    async def execute() -> str:
        """
        Get all the content for the wikipedia page you are currently on. Anything which corresponds to a link is wrapped in <link></link> tags.

        Args:
            None

        Returns:
            str: The content of the page with any accessible links wrapped in <link></link> tags
        """
        raise NotImplementedError("You need to implement the GetContentTool")
    return execute
     
@tool 
def MovePageTool(game : WikiGame) -> Tool:
    async def execute(page: str) -> str:
        """
        Move to a new wikipedia page by clicking on a link in the current page content. Modifies the game state in place.

        Args:
            page: The title of the page you want to move to. This must be accessible from the current page (and be a different page), or the move will fail.

        Returns:
            str: A message indicating whether the move was successful
        """ 
        raise NotImplementedError("You need to implement the MovePageTool")

    return execute
```


<details><summary> Hint: Regex for wrapping links </summary>

The code below describes how to wrap links, where `content` is the wikipedia page content, and `permitted_links` is the list of permitted links returned by our function earlier. 

```python
for word in sorted(permitted_links, key=len, reverse=True):
    content = re.sub(
        r"""(\s|[,.)!?;:'"])(""" + re.escape(word) + r""")(\s|[,.)!?;:'"s])""",
        r"\1<link>\2</link>\3",
        content,
        count=1,
        flags=re.IGNORECASE,
    )
```


</details>

<details><summary>Solution</summary>

```python
@tool
def GetContentTool(game : WikiGame) -> Tool:
    async def execute() -> str:
        """
        Get all the content for the wikipedia page you are currently on. Anything which corresponds to a link is wrapped in <link></link> tags.

        Args:
            None

        Returns:
            str: The content of the page with any accessible links wrapped in <link></link> tags
        """
        content = game.current_page.content
        permitted_links = get_permitted_links(game.current_page)
        for word in sorted(permitted_links, key=len, reverse=True):
            content = re.sub(
                r"""(\s|[,.)!?;:'"])(""" + re.escape(word) + r""")(\s|[,.)!?;:'"s])""",
                r"\1<link>\2</link>\3",
                content,
                count=1,
                flags=re.IGNORECASE,
            )
        return content
    return execute
     
@tool 
def MovePageTool(game : WikiGame) -> Tool:
    async def execute(page: str) -> str:
        """
        Move to a new wikipedia page by clicking on a link in the current page content. Modifies the game state in place.

        Args:
            page: The title of the page you want to move to. This must be accessible from the current page (and be a different page), or the move will fail.

        Returns:
            str: A message indicating whether the move was successful
        """ 
        page_no_underscore = page.replace("_", " ")
        if game.is_permitted_link(page):
            new_page = game.get_page(page)
            game.current_page = new_page
            return "Move successful"
        elif game.is_permitted_link(page_no_underscore):
            new_page = game.get_page(page_no_underscore)
            return "Move successful"
        else:
            return "Move failed, link not permitted. Remember you can only move to pages which are wrapped in <link></link> tags in the content you retrieved using the GetContentTool."

    return execute
```

</details>


### Exercise - Build a WikiAgent
> ```yaml
> Difficulty: 🔴🔴🔴🔴⚪
> Importance: 🔵🔵🔵🔵🔵
> 
> You should spend up to 30-60 mins on this exercise.
> ```

We will now build a `WikiAgent` that can use these tools to solve the `WikiGame`. We want to build the agent so that it can be called via an agent loop using the `execute()` method, similar to the one we had for the arithmetic game. 

There are a few further considerations in this case that we didn't have for the arithmetic game. 

#### Context window constraint

Since Wikipedia articles could be very long, the length of the LLM's context window becomes a constraint. GPT-4o and GPT-4o-mini both have context windows of 128k tokens (which corresponds to ~96k words). For reference, the wikipedia page for the United States has around 10k words alone and the agent will often need to visit more than 10 articles in one run of the game, not counting its own output, which can add up to be significant. 

We'll solve this for now by simply resetting the messages of the agent every time it reaches a new wikipedia page, and providing an updated set of instructions, so the agent can locate itself in the game. We'll cover different methods for addressing this issue later, you can probably already think of some. So be careful to include the current page and goal page for the agent in the instruction. For this reason you should write a `_reset_history()` function in the agent's definition. 


#### Variables and methods of this function
We've implemented one method for this function which isn't very conceptually important, this is:

- `reset_history()` - this is useful for dealing with context window constraints as mentioned above. This method should be called each time the model successfully moves to a new page, to clear it's current chat history.

You'll need to implement the following variables:
- `system_instruction` - this should return the system message as a `ChatMessageSystem` which we'll give to the model as it attempts the task. It should tell the model what the `WikiGame` is, and only the *basics* of how to play it.

- `on_page_instruction` - this should be a user message as a `ChatMessageUser` telling the model the specific page it is on, and the page it should try to reach.

- `next_step_instruction` - this should be a user message, also as a `ChatMessageUser`, that prompts the model to take its next action. This should be passed to the model after each time the model makes a tool call.

You'll also need to implement the following methods


- `_start()` - This should load the initial `system_instruction` and `on_page_instruction` into the `state.messages`, so that the model can start the `WikiGame` with the necessary instructions to know what to do.

- `instruction_refresh()` - this function should reset the instruction variables we defined. Then we can call it whenever we move page and update our WikiGame class. This should just reset the variables.

- `execute()` - This function should contain a loop that consists of our main agent logic. Within this loop, we should be making a call to the LLM API, and using the methods from inspect, or from the rest of the function to handle the response, whether the response is a tool call, or purely a text response.

- `_handle_tool_calls()` - This function should execute the model's tool calls, append the results to the message history, and perform any necessary post-tool processing (like refreshing instructions after a page move)


```python
@agent 
def WikiAgent(tools : list[Tool], game: WikiGame):
    system_instruction = 
    on_page_instruction =
    next_step_instruction =
    raise NotImplementedError("You need to implement the prompts for the WikiAgent")

    async def instruction_refresh() -> None:
        nonlocal system_instruction, on_page_instruction, next_step_instruction
        raise NotImplementedError("You need to implement the instruction_refresh function")
        

    async def _reset_history(state : AgentState):
        state.messages = []
        state = await _start(state)
        return state 
    async def _start(state: AgentState) -> AgentState:
        raise NotImplementedError("You need to implement the _start function")
    async def _handle_tool_calls(state: AgentState) -> AgentState:
        raise NotImplementedError("You need to implement the _handle_tool_calls function")
    async def execute(state : AgentState) -> AgentState:
        raise NotImplementedError("You need to implement the execute function")
    return execute
```


<details><summary>Solution</summary>

```python
@agent 
def WikiAgent(tools : list[Tool], game: WikiGame):
    system_instruction = ChatMessageSystem(content = "You are a wikipedia-racing AI. Your aim is to reach the goal page by accessing links from a series of wikipedia pages.")

    on_page_instruction = ChatMessageUser(content = f"You are currently on page: {game.current_page.title}. Your goal page is {game.goal_page.title}.")

    next_step_instruction = ChatMessageUser(content = "What will you do next?")

    async def instruction_refresh() -> None:
        nonlocal system_instruction, on_page_instruction, next_step_instruction
        system_instruction = ChatMessageSystem(content = "You are a wikipedia-racing AI. Your aim is to reach the goal page by accessing links from a series of wikipedia pages.")

        on_page_instruction = ChatMessageUser(content = f"You are currently on page: {game.current_page.title}. Your goal page is {game.goal_page.title}.")

        next_step_instruction = ChatMessageUser(content = "What will you do next?")
        

    async def _reset_history(state : AgentState):
        state.messages = []
        state = await _start(state)
        return state 
    async def _start(state: AgentState) -> AgentState:
        state.messages.append(system_instruction)
        state.messages.append(on_page_instruction)
        return state
    async def _handle_tool_calls(state: AgentState) -> AgentState:
        messages, state.output = await execute_tools(messages= state.messages, tools = tools)
        state.messages.extend(messages)
        if state.output.message.tool_calls[0].function == "MovePageTool" and "success" in messages[-1].content.lower():
            await instruction_refresh()
            state = await _reset_history(state)
        return state
    async def execute(state : AgentState) -> AgentState:
        success= False
        state = await _start(state)
        while not success:
            state.messages.append(next_step_instruction)
            state.output = await get_model().generate(
                    input=state.messages,                          
                    tools=tools,                                   
                )
            state.messages.append(state.output.message)
            if state.output.message.tool_calls:
                state = await _handle_tool_calls(state)
            if game.check_win():
                success = True
        return state 
    return execute
```
</details>


### Exercise - Run the task
> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵⚪⚪⚪
> 
> You should spend up to 10-15 mins on this exercise.
> ```


Now, similar to how we ran the `arithmetic_agent`, use the eval function to run the `wikipedia_agent` on the task below. This time is slightly different, as we need to define our tool_list, since our tools take our game as an argument (and we need to make sure they're accessing the same game as the agent, otherwise their execution will be incorrect). We also make sure we include a `message_limit`, (40 should be a fine message limit to start with) so that the agent doesn't run forever.


```python
game = WikiGame("Python (programming language)", "Artificial intelligence")
# Use the eval function to evaluate your WikiAgent on a task where it has to get from the "Python (programming language)" page to the "Artificial intelligence" page.
```


<details><summary>Solution</summary>

```python
game = WikiGame("Python (programming language)", "Artificial intelligence")
tool_list = [GetContentTool(game), MovePageTool(game)]

@task 
def wiki_task() -> Task:
    return Task(dataset = [Sample(input = "", target = "")], message_limit=20)

eval(solver = as_solver(WikiAgent(tools = tool_list, game = game)), tasks = wiki_task(),)
```
</details>


Your agent should be able to accomplish the following tasks. If the agent fails on the first try, then run the agent again (we've tried to cut down on random behaviour by the agents by setting the temperature to 0, however OpenAI's models retain some randomness at temperature 0 which compounds as they proceed through the task).


```python
game_1 = WikiGame("Elizabeth I", "United States")
tool_list = [GetContentTool(game_1), MovePageTool(game_1)]
@task
def wiki_task() -> Task:
    return Task(dataset = [Sample(input = "", target = "")], message_limit=80)
eval(solver = as_solver(WikiAgent(tools = tool_list, game = game_1)), tasks = wiki_task(),)
```


```python
game_2 = WikiGame("County Seat", "Saint Pierre and Miquelon")
tool_list = [GetContentTool(game_2), MovePageTool(game_2)]
@task
def wiki_task() -> Task:
    return Task(dataset = [Sample(input = "", target = "")], message_limit=80)
eval(solver = as_solver(WikiAgent(tools = tool_list, game = game_2)), tasks = wiki_task(),)
```




=== NEW CHAPTER ===


# 4️⃣ Elicitation



> ##### Learning Objectives
>
> - Understand the different methods of elicitation
> - Understand how to improve prompting, tools, history storage, and information access in LLM agents

You may have observed that while our initial implementation of `WikiAgent` succeeds at these relatively challenging games, if we increase the difficulty slightly, then the agent will fail (one possible example is the game: Joinery → Amethyst; our agent will usually fail on this task). However, this doesn't mean that GPT-4o-mini does not have the capability to perform better on this task, but this capability might be blocked because we:

- Prompted the model poorly or ineffectively.
- Stored and presented the task history poorly.
- Didn't give the model sufficient tools to accomplish the task.

In general, it is hard to show that a model does not have a certain capability, even if we've failed to *demonstrate* this capability. For example, it took 3.5 years after the release of GPT-2 (and 1.5 years after the release of GPT-3) for people to discover that [chain-of-thought reasoning](https://arxiv.org/abs/2201.11903) massively improves model performance, which enabled the same models to complete significantly harder tasks. Dangerous capability evaluations for LLM agents require us to elicit the best capabilities possible, until we feel we've managed to gain [**evidence of absence**](https://en.wikipedia.org/wiki/Evidence_of_absence), **not** just **absence of evidence**.


Broadly speaking, there are two categories of elicitation:

1. **Narrow elicitation**: Task-specific methods that improve model performance on a particular task or small class of tasks, but likely won't impact model performance in general across many tasks. 
    - E.g. A tool that gives the model access to the content of arbitrary wikipedia articles. This will improve performance on this task significantly, but wouldn't generalize to other tasks.
2. **General elicitation**: Task-agnostic methods that improve model performance on a wide array of possible tasks. 
    - E.g. Chain-of-thought prompting: This tends to improve model performance on a wide array of tasks. These sorts of elicitation methods are the ones we're most interested in. If researchers find an improvement to models that is roughly as easy and effective as chain-of-thought prompting, then we would see a very rapid increase in risk from AI.


The elicitation methods we'll try in this section will mostly revolve around prompting in order to obtain better performance, including chain-of-thought prompting, the ReAct Framework; as well as some more exotic methods, like a lookahead tool.

<details><summary>Tip - How to find wikipedia pages to test on</summary>

You might start having a hard time coming up with wikipedia pages to test on. Luckily, there are websites which generate random pages for this purpose, one good website is accessible via: https://wikispeedruns.com/ (you may want to change the "Random Article Generator Settings" to sample from the most popular 100,000 wikipedia pages, as the default setting of 3000 will generally generate paths that are too easy to test our agent). We've also provided you with a list of 18 wikipedia pairs, stored as `wiki_pairs`. These are ordered approximately in increasing difficulty.

To test whether two pages are connected via links, use this free online tool to see the possible paths between pages: https://www.sixdegreesofwikipedia.com/ (be somewhat careful with this though, as the paths that this website believes are accessible may not be accessible to our agent).

</details>

In this section, we'll use the `gpt-4o-mini-2024-07-18` model to gauge whether our elicitation methods are effective since OpenAI will occasionally release small updates to `gpt-4o-mini` which change its behaviour. However, if you're curious, you can try testing your elicitation methods on the newest `gpt-4o-mini` model. What you will most likely notice is that your elicitation methods improve the model significantly less, and the model performs much better at the task without needing as much elicitation. This is because their most recent models are generally more capable, and so saturate the evaluation of "How well can a model play the Wikipedia game" faster. For a real agent evaluation, you'd want to have increasingly difficult tasks, so that even as models improve, we can find tasks that are too difficult for them to achieve (e.g. the 16-hour tasks on METR's [Measuring AI Ability to Complete Long Tasks](https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/))


```python
os.environ["INSPECT_EVAL_MODEL"] = "openai/gpt-4o-mini-2024-07-18"
```


As you should already know, prompting can have a large impact on model performance. There are many changes you could make for prompts in this task. You should experiment first with more general elicitation methods such as getting the agent to think more deeply, and output plans in different ways. After this, you might try more narrow elicitation methods, such as:

- Telling the agent how many pages it's visited.
- Telling the agent if it's already visited the page it's on (and how many times).
- Schedule different prompts and planning methods for the "zoom out" and "zoom in" sections of the game, since we know that a good general strategy for playing the wikipedia game is:

   `Narrow article (with few links) -> General article (with many links) -> Narrow article (with few links)`


### Exercise - Engineer prompts
> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵🔵⚪⚪
> 
> You should spend up to 20-35 mins on this exercise.
> ```
Try and design prompts that improve the performance of the wikipedia agent. You may have to do a decent amount of experimentation here. Remember that your prompts will have to be robust to: 

* Different tasks within the wikipedia game, 
* Different states within those tasks,
* Different failure-modes the agent could encounter.

The rest of your agent should be defined the same way.

See if you can significantly improve performance. There's a test task below that you should aim to be able to solve with improved prompting. You'll need to modify the `system_instruction`, `on_page_instruction` and `next_step_instruction`, and don't forget to also modify the `instruction_refresh()` function, otherwise your prompts will revert to their old version every time the agent visits a new page.


```python
@agent
def WikiAgentPrompting(tools: list[Tool], game: WikiGame) -> Agent:
    system_instruction =
    on_page_instruction =
    next_step_instruction =
    raise NotImplementedError("You need to implement the prompts for the WikiAgentPrompting")

    async def _reset_history(state : AgentState):
        state.messages = []
        state = await _start(state)
        return state
    async def instruction_refresh() -> None:
        nonlocal system_instruction, on_page_instruction, next_step_instruction
        raise NotImplementedError("You need to reimplement the instruction_refresh function 
        ")
    async def _start(state: AgentState) -> AgentState:
        raise NotImplementedError("You need to implement the next_step_instruction function")
    async def _reset_history(state : AgentState):
        state.messages = []
        state = await _start(state)
        return state
    async def _start(state: AgentState) -> AgentState:
        raise NotImplementedError("You need to implement the _start function (copy your solution from before)")
    async def _handle_tool_calls(state: AgentState) -> AgentState:
        raise NotImplementedError("You need to implement the _handle_tool_calls function (copy your solution from before)")
    async def execute(state : AgentState) -> AgentState:
        raise NotImplementedError("You need to implement the execute function (copy your solution from before)")
    return execute
```


<details><summary>Solution</summary>

This isn't a *perfect* solution, but is an example of improved prompting compared to that in the original `WikiGame` class solution code. You may be able to do even better!
```python
@agent
def WikiAgentPrompting(tools: list[Tool], game: WikiGame) -> Agent:
    system_instruction = ChatMessageSystem(content = f"You are a wikipedia-racing AI. Your goal is to reach {game.goal_page.title} by accessing links from wikipedia pages. Your current page is {game.current_page.title}.")

    on_page_instruction = ChatMessageUser(content = f"""You are currently on page: {game.current_page.title}. Make sure you start by reasoning about what steps you should take to get to the article on {game.goal_page.title}. When coming up with a strategy, make sure to pay attention to the path you have already taken, and if your current strategy doesn't seem to be working out, try something else. In case you're unsure, {game.goal_page.title} has the following summary:\n\n[Begin Summary]\n{game.get_page_summary(game.goal_page)}\n[End Summary]\n\nThe path you have taken so far is {" -> ".join(game.page_history)}.
            """)

    next_step_instruction = ChatMessageUser(content = f"""What's your next step to reach {game.goal_page.title}? Make sure to think carefully about what steps you should take to get there.""")

    async def _reset_history(state : AgentState):
        state.messages = []
        state = await _start(state)
        return state
    async def instruction_refresh() -> None:
        nonlocal system_instruction, on_page_instruction, next_step_instruction
        system_instruction = ChatMessageSystem(content = f"You are a wikipedia-racing AI. Your goal is to reach {game.goal_page.title} by accessing links from wikipedia pages. Your current page is {game.current_page.title}.")

        on_page_instruction = ChatMessageUser(content = f"""You are currently on page: {game.current_page.title}. Make sure you start by reasoning about what steps you should take to get to the article on {game.goal_page.title}. When coming up with a strategy, make sure to pay attention to the path you have already taken, and if your current strategy doesn't seem to be working out, try something else. In case you're unsure, {game.goal_page.title} has the following summary:\n\n[Begin Summary]\n{game.get_page_summary(game.goal_page)}\n[End Summary]\n\nThe path you have taken so far is {" -> ".join(game.page_history)}.
                """)

        next_step_instruction = ChatMessageUser(content = f"""What's your next step to reach {game.goal_page.title}? Make sure to think carefully about what steps you should take to get there.""")
    async def _start(state: AgentState) -> AgentState:
        state.messages.append(system_instruction)
        state.messages.append(on_page_instruction)
        return state
    async def _reset_history(state : AgentState):
        state.messages = []
        state = await _start(state)
        return state
    async def _start(state: AgentState) -> AgentState:
        state.messages.append(system_instruction)
        state.messages.append(on_page_instruction)
        return state
    async def _handle_tool_calls(state: AgentState) -> AgentState:
        messages, state.output = await execute_tools(messages= state.messages, tools = tools)
        state.messages.extend(messages)
        if state.output.message.tool_calls[0].function == "MovePageTool" and "success" in messages[-1].content.lower():
            await instruction_refresh()
            state = await _reset_history(state)
        return state
    async def execute(state : AgentState) -> AgentState:
        success= False
        state = await _start(state)
        while not success:
            state.messages.append(next_step_instruction)
            state.output = await get_model().generate(
                    input=state.messages,
                    tools=tools,
                )
            state.messages.append(state.output.message)
            if state.output.message.tool_calls:
                state = await _handle_tool_calls(state)
            if game.check_win():
                success = True
        return state 
    return execute
```

</details>


LLM agents can be quite random - as you might have noticed - as a result of the default temperature being 1, and agents operating over a much longer horizon than usual for LLMs. So we'll do our testing at `temperature = 0`. This impacts performance noticeably, but better elicitation methods still have a noticeable effect.

Your original `WikiAgent` may not reliably be able to solve the example path `Mandate of Heaven -> Doric Greek` at temperature 0 (although it may occasionally get lucky). However, with sufficiently improved prompting, you should be able to get the agent to solve this task reliably.


```python
game = WikiGame("Mandate of Heaven", "Doric Greek")
tool_list = [GetContentTool(game), MovePageTool(game)]
@task
def wiki_task() -> Task:
    return Task(dataset = [Sample(input = "", target = "")], message_limit=80)
eval(solver = as_solver(WikiAgent(tools = tool_list, game = game)), tasks = wiki_task(),)
```


```python
game = WikiGame("Mandate of Heaven", "Doric Greek")
tool_list = [GetContentTool(game), MovePageTool(game)]
@task
def wiki_task() -> Task:
    return Task(dataset = [Sample(input = "", target = "")], message_limit=80)
eval(solver = as_solver(WikiAgentPrompting(tools = tool_list, game = game)), tasks = wiki_task(),)
```


### Exercise - Implement the ReAct framework
> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵🔵⚪⚪
> 
> You should spend up to 15-20 mins on this exercise.
> ```
The [**ReAct** framework](https://arxiv.org/abs/2210.03629) is an extension of chain-of-thought reasoning. Instead of prompting the model to think step-by-step, it separates this into two steps, especially designed for agent-based tasks:

- **Re**asoning: The model is asked to reason about its current situation, and what sort of actions it should consider taking.
- **Act**ion: Then, the model is asked to perform an action based on its outputted reasoning.

We'll need to write new prompts to indicate that the model should think during the reasoning step, and that the model should act during the action step. During the reasoning step, we don't want the model to make any tool calls, so we can use the `tool_choice` argument of `generate()` -- by default this is set to `"auto"`, but since we want no tools to be called, we should set `tool_choice="none"`.

Here you'll need to implement the `generate_reason` and `generate_action` functions, and reimplement `execute()` to work in the ReAct framework. Otherwise you should copy your solutions from before.


```python
@agent
def WikiAgentReAct(tools: list[Tool], game: WikiGame) -> Agent:

    system_instruction =
    on_page_instruction =
    raise NotImplementedError("You need to implement the prompts for the WikiAgentReAct (copy your solutions from before). You likely won't need the next_step_instruction this time.")
    system_instruction = ChatMessageSystem(content = f"You are a wikipedia-racing AI. Your goal is to reach {game.goal_page.title} by accessing links from wikipedia pages. Your current page is {game.current_page.title}.")

    on_page_instruction = ChatMessageUser(content = f"""You are currently on page: {game.current_page.title}. Make sure you start by reasoning about what steps you should take to get to the article on {game.goal_page.title}. When coming up with a strategy, make sure to pay attention to the path you have already taken, and if your current strategy doesn't seem to be working out, try something else. In case you're unsure, {game.goal_page.title} has the following summary:\n\n[Begin Summary]\n{game.get_page_summary(game.goal_page)}\n[End Summary]\n\nThe path you have taken so far is {" -> ".join(game.page_history)}.
            """)
    async def _reset_history(state : AgentState):
        state.messages = []
        state = await _start(state)
        return state
    
    async def instruction_refresh() -> None:
        nonlocal system_instruction, on_page_instruction
        raise NotImplementedError("You need to reimplement the instruction_refresh function (copy your solution from before)")

    async def generate_reason(state : AgentState) -> AgentState:
        raise NotImplementedError("You need to implement the generate_reason function")
    async def generate_action(state : AgentState) -> AgentState:
        raise NotImplementedError("You need to implement the generate_action function")
    async def _start(state: AgentState) -> AgentState:
        raise NotImplementedError("You need to implement the _start function (copy your solution from before)")
    async def _handle_tool_calls(state: AgentState) -> AgentState:
        raise NotImplementedError("You need to implement the _handle_tool_calls function (copy your solution from before)")
    async def execute(state : AgentState) -> AgentState:
        raise NotImplementedError("You need to reimplement the execute function")
    return execute
```


<details><summary>Solution</summary>

```python
@agent
def WikiAgentReAct(tools: list[Tool], game: WikiGame) -> Agent:

    system_instruction = ChatMessageSystem(content = f"You are a wikipedia-racing AI. Your goal is to reach {game.goal_page.title} by accessing links from wikipedia pages. Your current page is {game.current_page.title}.")

    on_page_instruction = ChatMessageUser(content = f"""You are currently on page: {game.current_page.title}. Make sure you start by reasoning about what steps you should take to get to the article on {game.goal_page.title}. When coming up with a strategy, make sure to pay attention to the path you have already taken, and if your current strategy doesn't seem to be working out, try something else. In case you're unsure, {game.goal_page.title} has the following summary:\n\n[Begin Summary]\n{game.get_page_summary(game.goal_page)}\n[End Summary]\n\nThe path you have taken so far is {" -> ".join(game.page_history)}.
            """)
    async def _reset_history(state : AgentState):
        state.messages = []
        state = await _start(state)
        return state
    
    async def instruction_refresh() -> None:
        nonlocal system_instruction, on_page_instruction
        system_instruction = ChatMessageSystem(content = f"You are a wikipedia-racing AI. Your goal is to reach {game.goal_page.title} by accessing links from wikipedia pages. Your current page is {game.current_page.title}.")

        on_page_instruction = ChatMessageUser(content = f"""You are currently on page: {game.current_page.title}. Make sure you start by reasoning about what steps you should take to get to the article on {game.goal_page.title}. When coming up with a strategy, make sure to pay attention to the path you have already taken, and if your current strategy doesn't seem to be working out, try something else. In case you're unsure, {game.goal_page.title} has the following summary:\n\n[Begin Summary]\n{game.get_page_summary(game.goal_page)}\n[End Summary]\n\nThe path you have taken so far is {" -> ".join(game.page_history)}.
                """)

    async def generate_reason(state : AgentState) -> AgentState:
        state.messages.append(ChatMessageUser(content = f"""Before you decide on your next step, think carefully about what steps you should take to get to {game.goal_page.title}. When coming up with a strategy, make sure to pay attention to the path you have already taken, and if your current strategy doesn't seem to be working out, try something else."""))
        state.output = await get_model().generate(
                    input=state.messages,
                    tools=tools,
                    tool_choice="none"
                )
        state.messages.append(state.output.message)
        return state
    async def generate_action(state : AgentState) -> AgentState:
        state.messages.append(ChatMessageUser(content = f"""Now based on your reasoning above, what action will you take to reach {game.goal_page.title}?"""))
        state.output = await get_model().generate(
                    input=state.messages,
                    tools=tools,
                    tool_choice = "auto"
                )
        state.messages.append(state.output.message)
        return state
    async def _start(state: AgentState) -> AgentState:
        state.messages.append(system_instruction)
        state.messages.append(on_page_instruction)
        return state
    async def _handle_tool_calls(state: AgentState) -> AgentState:
        messages, state.output = await execute_tools(messages= state.messages, tools = tools)
        state.messages.extend(messages)
        if state.output.message.tool_calls[0].function == "MovePageTool":
            await instruction_refresh()
            state = await _reset_history(state)
        return state
    async def execute(state : AgentState) -> AgentState:
        success= False
        state = await _start(state)
        while not success:
            state = await generate_reason(state) 
            state = await generate_action(state)
            if state.output.message.tool_calls:
                state = await _handle_tool_calls(state)
            if game.check_win():
                success = True
        return state 
    return execute
```
</details>


Now run your Wikipedia ReAct agent (we haven't given tests to check that the model works, since your precise implementation may deviate from ours, by running the agent, and checking its `chat_history`, you should be able to tell whether your ReAct framework is operating correctly). You should be able to notice an improved reasoning process each time the model runs, and might notice that on some paths the model performs more effectively (although this is hard to demonstrate conclusively).

However, you'll also likely notice that the difference between effective prompting, and the ReAct method we've implemented here isn't massive. Using the ReAct framework is similar to chain-of-thought prompting in this case, and prompting can make a difference only up to a point. However, ReAct does tend to make the agent more reliable on higher temperatures (although this is impossible to identify in just a single run).


```python
# Run the game with WikiAgentPrompting
game = WikiGame("Balto-Slavic languages", "Netscape Navigator 9")
tool_list = [GetContentTool(game), MovePageTool(game)]
@task
def wiki_task() -> Task:
    return Task(dataset = [Sample(input = "", target = "")], message_limit=80)

eval(solver = as_solver(WikiAgentPrompting(tools = tool_list, game = game)), tasks = wiki_task(),)
```


```python
# Run the game with WikiAgentReAct
game = WikiGame("Balto-Slavic languages", "Netscape Navigator 9")
tool_list = [GetContentTool(game), MovePageTool(game)]
@task
def wiki_task() -> Task:
    return Task(dataset = [Sample(input = "", target = "")], message_limit=80)

eval(solver = as_solver(WikiAgentReAct(tools = tool_list, game = game)), tasks = wiki_task(),)
```


### Exercise - Let the LLM see its entire chat history
> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵⚪⚪⚪
> 
> You should spend up to 10-15 mins on this exercise.
> ```

You may have noticed that the agent performs significantly worse as a result of the fact that we decided to reset the chat history every time the agent encounters a new page. For example, it will occasionally come up with good plans and not follow through on them, since its in-context memory has been erased. We can fix this issue by letting the agent see the entirety of its chat history.

The main obstacle to allowing the agent to see its entire history is the capacity of its context window -- specifically due to the length of wikipedia articles that the agent has to retrieve in order to play the game. However, we can fix this issue by resetting **only** the outputs of the `GetContentTool` function each time the agent moves to a new page, instead of resetting the entire chat history.

When we reset this content, we should still let the agent know that Wikipedia content was output in that location, as otherwise it will confuse the agent about the `get_content` tool. You should replace the content with `"Wikipedia content was output here. Wikipedia page: {page_title}"` so that the agent knows that the `get_content` tool works as expected. 
 
Modify the `_reset_history` function in the `WikiAgentHistory` function below to accomplish this. You may also need to modify the `_handle_tool_calls` function to keep track of the previous page.


```python
@agent 
def WikiAgentHistory(tools : list[Tool], game: WikiGame, verbose : bool = True):
    system_instruction =
    on_page_instruction =
    raise NotImplementedError("You need to implement the prompts for the WikiAgentHistory (copy your solutions from before).")
    async def instruction_refresh() -> None:
        nonlocal system_instruction, on_page_instruction
        raise NotImplementedError("You need to reimplement the instruction_refresh function (copy your solution from before)")
    async def _reset_history(state : AgentState, previous_page : str) -> AgentState:
        raise NotImplementedError("You need to implement the new _reset_history function")
    async def generate_reason(state : AgentState) -> AgentState:
        raise NotImplementedError("You need to implement the generate_reason function")
    async def generate_action(state : AgentState) -> AgentState:
        raise NotImplementedError("You need to implement the generate_action function")
    async def _start(state: AgentState) -> AgentState:
        raise NotImplementedError("You need to implement the _start function (copy your solution from before)")
    async def _handle_tool_calls(state: AgentState) -> AgentState:
        raise NotImplementedError("You need to reimplement the _handle_tool_calls function")
    async def execute(state : AgentState) -> AgentState:
        raise NotImplementedError("You need to implement the execute function (copy your solution from before)")
    return execute
```


<details><summary>Solution</summary>

```python
@agent 
def WikiAgentHistory(tools : list[Tool], game: WikiGame, verbose : bool = True):
    system_instruction = ChatMessageSystem(content = f"You are a wikipedia-racing AI. Your goal is to reach {game.goal_page.title} by accessing links from wikipedia pages. Your current page is {game.current_page.title}.")

    on_page_instruction = ChatMessageUser(content = f"""You are currently on page: {game.current_page.title}. Make sure you start by reasoning about what steps you should take to get to the article on {game.goal_page.title}. When coming up with a strategy, make sure to pay attention to the path you have already taken, and if your current strategy doesn't seem to be working out, try something else. In case you're unsure, {game.goal_page.title} has the following summary:\n\n[Begin Summary]\n{game.get_page_summary(game.goal_page)}\n[End Summary]\n\nThe path you have taken so far is {" -> ".join(game.page_history)}.
            """)
    async def instruction_refresh() -> None:
        nonlocal system_instruction, on_page_instruction
        system_instruction = ChatMessageSystem(content = f"You are a wikipedia-racing AI. Your goal is to reach {game.goal_page.title} by accessing links from wikipedia pages. Your current page is {game.current_page.title}.")
        on_page_instruction = ChatMessageUser(content = f"""You are currently on page: {game.current_page.title}. Make sure you start by reasoning about what steps you should take to get to the article on {game.goal_page.title}. When coming up with a strategy, make sure to pay attention to the path you have already taken, and if your current strategy doesn't seem to be working out, try something else. In case you're unsure, {game.goal_page.title} has the following summary:\n\n[Begin Summary]\n{game.get_page_summary(game.goal_page)}\n[End Summary]\n\nThe path you have taken so far is {" -> ".join(game.page_history)}.
                """)
    async def _reset_history(state : AgentState, previous_page : str) -> AgentState:
        for message in state.messages:
            if isinstance(message, ChatMessageTool) and message.function == "GetContentTool" and "Wikipedia page content for page" not in message.content:
                message.content = f"Wikipedia page content for page {previous_page} was output here, but has been removed for brevity."
        return state
    async def generate_reason(state : AgentState) -> AgentState:
        state.messages.append(ChatMessageUser(content = f"""Before you decide on your next step, think carefully about what steps you should take to get to {game.goal_page.title}. When coming up with a strategy, make sure to pay attention to the path you have already taken, and if your current strategy doesn't seem to be working out, try something else."""))
        state.output = await get_model().generate(
                    input=state.messages,
                    tools=tools,
                    tool_choice="none"
                )
        state.messages.append(state.output.message)
        return state
    async def generate_action(state : AgentState) -> AgentState:
        state.messages.append(ChatMessageUser(content = f"""Now based on your reasoning above, what action will you take to reach {game.goal_page.title}?"""))
        state.output = await get_model().generate(
                    input=state.messages,
                    tools=tools,
                    tool_choice = "auto"
                )
        state.messages.append(state.output.message)
        return state
    async def _start(state: AgentState) -> AgentState:
        state.messages.append(system_instruction)
        state.messages.append(on_page_instruction)
        return state
    async def _handle_tool_calls(state: AgentState) -> AgentState:
        messages, state.output = await execute_tools(messages= state.messages, tools = tools)
        state.messages.extend(messages)
        if state.output.message.tool_calls[0].function == "MovePageTool" and "success" in messages[-1].content.lower():
            previous_page = game.current_page.title
            await instruction_refresh()
            state = await _reset_history(state, previous_page)
        return state
    async def execute(state : AgentState) -> AgentState:
        success= False
        state = await _start(state)
        while not success:
            state = await generate_reason(state) 
            state = await generate_action(state)
            if state.output.message.tool_calls:
                state = await _handle_tool_calls(state)
            if game.check_win():
                success = True
        return state 
    return execute
```
</details>


Now let's test our new agent on a different path, we've proposed one below, but feel free to try it on an example path of your own choosing, and see how it performs compared to our previous `WikiAgentReAct`.


```python
game = WikiGame("Blavatnik School of Government", "Free Thai Movement")
tool_list = [GetContentTool(game), MovePageTool(game)]

@task 
def wiki_task() -> Task:
    return Task(dataset = [Sample(input = "", target = "")], message_limit=120)

eval(solver = as_solver(WikiAgentReAct(tools = tool_list, game = game)), tasks = wiki_task(),)
```


```python
game = WikiGame("Blavatnik School of Government", "Free Thai Movement")
tool_list = [GetContentTool(game), MovePageTool(game)]

@task 
def wiki_task() -> Task:
    return Task(dataset = [Sample(input = "", target = "")], message_limit=120)

eval(solver = as_solver(WikiAgentHistory(tools = tool_list, game = game)), tasks = wiki_task(),)
```


### Exercise - Implement a reflexion tool
> ```yaml
> Difficulty: 🔴🔴🔴⚪⚪
> Importance: 🔵🔵🔵⚪⚪
> 
> You should spend up to 25-35 mins on this exercise.
> ```

The [reflexion paper](https://arxiv.org/abs/2303.11366) proposes a method that improves performance by getting LLMs to do self-reflection. The original paper looks at LLM agents in a RL set-up, where getting a reward signal on the agent's signal is slow and expensive. The key idea is to get **quick cheap feedback** from an evaluator on every proposed action, then to **reflect** on this feedback before taking the next action, as opposed to waiting for the final outcome. In their case, the evaluator was a heuristic function that estimated the reward function. 

We will borrow and modify this idea by building a tool that allows our agent to perform a lookahead, and then gives feedback on our agent's proposed actions. We allow the agent to suggest candidate paths, then the tool will check if these paths work and inform the model either that the path works, or where the path goes wrong.

We don't want to provide the agent the links or content of every page when it does this lookahead, as then we'd just be reimplementing a smaller version of the game *inside the game*. Instead, we'll let the agent suggest paths without seeing any content or links, and then let it know if this path works. It's very likely that a suggested link will — at some point — not be accessible from one of the pages, but this tool will still be useful to help the agent plan.


```python
@tool
def TestPathTool(game : WikiGame) -> Tool:
    async def execute(path: str) -> str:
        """
        Test a path of wikipedia pages to see if it leads to the goal page. The path should be a series of page titles separated by '->'. 

        Args:
            path (str): The path to test formatted as a series of wikipedia page titles separated by '->'. The path must start with the current page title. The path doesn't have to end with the goal page title.

        Returns:
            str: The result of the test. Success if the path leads to the goal page. Otherwise returns failure, and where the path failed.
        """
        raise NotImplementedError("You need to implement the TestPathTool function")
    return execute
```


```python
import os
game = WikiGame("Blavatnik School of Government", "Free Thai Movement")
tool_list = [GetContentTool(game), MovePageTool(game), TestPathTool(game)]
@task   
def wiki_task() -> Task:
    return Task(dataset = [Sample(input = "", target = "")], message_limit=120)

eval(solver = as_solver(WikiAgentHistory(tools = tool_list, game = game)), tasks = wiki_task())
```


<details><summary>Solution</summary>

```python
@tool
def TestPathTool(game : WikiGame) -> Tool:
    async def execute(path: str) -> str:
        """
        Test a path of wikipedia pages to see if it leads to the goal page. The path should be a series of page titles separated by '->'. 

        Args:
            path (str): The path to test formatted as a series of wikipedia page titles separated by '->'. The path must start with the current page title. The path doesn't have to end with the goal page title.

        Returns:
            str: The result of the test. Success if the path leads to the goal page. Otherwise returns failure, and where the path failed.
        """
        # Split the path into individual page titles
        path_nodes = [node.strip() for node in path.split("->")]
        if not path_nodes:
            return "Failure: Path is empty."
        if len(path_nodes) == 1:
            return "Failure: Path must contain at least two pages (current and goal)."
        if path_nodes[0].lower() != game.current_page.title.lower():
            return f"Failure: Path must start with the current page '{game.current_page.title}'."
        for i in range(len(path_nodes)-1):
            current_node = path_nodes[i]
            next_node = path_nodes[i+1]
            permitted_links = (link.lower() for link in get_permitted_links(get_page(current_node)))
            if next_node.lower() not in permitted_links:
                return f"This path works until {next_node}, which is not a permitted link on the page {current_node}."
        return "Success! Following this path will work successfully."
    return execute
```
</details>


You'll likely see that the agent often doesn't use this tool effectively, and when it does, will make suboptimal decisions based on the output of this tool:

 - One common failure mode is that the model will try a promising path, be told by the tool that it goes wrong *somewhere*, and then abandon the entire path for a much less promising path (without doing any further testing). 
 - Another common issue is that the agent will only use the tool to test whether it is possible to move a single page ahead, which is not the intended use of the tool (as the agent should be able to work out which pages it can move to in one step by looking at the page's content). 

Although it may be tempting to continue adding additional tools to agents, if they're not capable of using them correctly and effectively, then these tools may actually harm performance. There are tasks where a 'lookahead' tool could be used effectively, however it turns out that the Wikipedia game task isn't one of them.




=== NEW CHAPTER ===


# 5️⃣ Bonus


In this bonus section, we'll suggest some modifcations to the wikipedia game to make it more difficult so that you can then go on and try further elicitation methods of your own.

Alternatively, if you're tired of the Wikipedia game, and are feeling ambitious, you might want to try designing your own agent task, and quantifying performance on that task.


### Exercise - Implement additional rules
> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵⚪⚪⚪⚪
> ```

Allow the game to have additional rules. Some suggestions are a "No country pages" rule, and a "No articles above a given length" rule, but feel free to add more. With all of our elicitation methods, the agent generally only fails if the path is impossible or unreasonably hard. To implement a no country rule, you may want to use the wikipedia API's "categories" attribute for `WikipediaPage` objects.

First, let's modify the `WikiGame` task to store the rules for the Wikipedia game. We've modified the class for you to allow for the rules we described above.


```python
class WikiGameRules(WikiGame):
    def __init__(
        self,
        starting_page: str,
        goal_page: str,
        rules: Optional[list[Literal["no countries", "no pages with length above 30000"]]],
    ):
        super().__init__(starting_page, goal_page)
        self.rules = rules
```


Now let's modify the prompts given to the LLM API in the Agent function so that we inform the agent about any additional rules it will have to abide by. We should have the option to maintain our original prompts (in case we decide to run the WikiAgent without any rules), so the new `system_instruction` method should first check whether there are any additional rules, and only return the modified system prompt if there are.


Now you'll have to implement these rules by modifying the `MovePageTool` class, so that the agent can only move page if it's within the rules of the game. If you're running the agent with the reflexion tool, you may also want to modify the logic of that tool to abide by the rules.


```python
# Implement modified MovePageTool to enforce rules here.
```


### Try further elicitation methods

Read some further resources on building and eliciting behaviour from LLM agents, and try implementing some of your own methods to elicit improved performance on the task. If you start seeing diminishing returns from elicitation (due to saturating performance on the task), come up with new ways to make the task harder. Alternatively, if you're feeling particularly ambitious, you can try and come up with your own more difficult task and build an agent to try and accomplish this from scratch.

